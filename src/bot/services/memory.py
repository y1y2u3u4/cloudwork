"""
CloudWork Memory Service

ä¸‰å±‚è®°å¿†ç®¡ç†ç³»ç»Ÿ:
- Layer 1: çŸ­æœŸè®°å¿† (daily/) â€” æ¯æ—¥ä¼šè¯æ‘˜è¦
- Layer 2: ä¸­æœŸè®°å¿† (learned/) â€” å¯å¤ç”¨æŠ€æœ¯æ¨¡å¼
- Layer 3: é•¿æœŸè®°å¿† (MEMORY.md) â€” ç”¨æˆ·åå¥½å’Œé¡¹ç›®çŸ¥è¯†

æŒç»­å­¦ä¹ åŠŸèƒ½:
- ä¼šè¯ç»“æŸæ—¶è‡ªåŠ¨æå–æœ‰ä»·å€¼çš„æ¨¡å¼
- è¯†åˆ«: é”™è¯¯è§£å†³æ–¹æ¡ˆã€ç”¨æˆ·çº æ­£ã€å˜é€šæ–¹æ¡ˆã€è°ƒè¯•æŠ€å·§

è®°å¿†é—å¿˜æœºåˆ¶:
- è‡ªåŠ¨æ¸…ç†ä½ä»·å€¼çš„æ—§è®°å¿†
- åŸºäº: æ—¶é—´ã€é‡è¦æ€§ã€è®¿é—®é¢‘ç‡
"""

import os
import logging
import re
import asyncio
import subprocess
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)


class MemoryManager:
    """ä¸‰å±‚è®°å¿†ç®¡ç†å™¨"""

    def __init__(self, data_dir: str):
        """
        åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨

        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„ (é€šå¸¸æ˜¯ cloudwork/data)
        """
        self.data_dir = Path(data_dir)
        self.memory_dir = self.data_dir / "memory"
        self.daily_dir = self.memory_dir / "daily"
        self.learned_dir = self.memory_dir / "learned"
        self.archive_dir = self.daily_dir / "archive"
        self.memory_file = self.memory_dir / "MEMORY.md"
        self.index_file = self.memory_dir / "index.md"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self._ensure_dirs()

    def _ensure_dirs(self):
        """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        self.daily_dir.mkdir(parents=True, exist_ok=True)
        self.learned_dir.mkdir(parents=True, exist_ok=True)
        self.archive_dir.mkdir(parents=True, exist_ok=True)

    # ============ Layer 1: çŸ­æœŸè®°å¿† (Daily) ============

    def get_daily_file(self, date: Optional[datetime] = None) -> Path:
        """è·å–æŒ‡å®šæ—¥æœŸçš„è®°å¿†æ–‡ä»¶è·¯å¾„"""
        if date is None:
            date = datetime.now()
        return self.daily_dir / f"{date.strftime('%Y-%m-%d')}.md"

    def load_daily(self, date: Optional[datetime] = None) -> str:
        """åŠ è½½æŒ‡å®šæ—¥æœŸçš„è®°å¿†"""
        filepath = self.get_daily_file(date)
        if filepath.exists():
            return filepath.read_text(encoding='utf-8')
        return ""

    def save_daily(self, content: str, date: Optional[datetime] = None):
        """ä¿å­˜æ¯æ—¥è®°å¿†"""
        filepath = self.get_daily_file(date)
        filepath.write_text(content, encoding='utf-8')
        logger.info(f"Saved daily memory: {filepath.name}")

    def append_daily(self, section: str, content: str, date: Optional[datetime] = None):
        """è¿½åŠ å†…å®¹åˆ°æ¯æ—¥è®°å¿†çš„æŒ‡å®š section"""
        filepath = self.get_daily_file(date)

        if filepath.exists():
            existing = filepath.read_text(encoding='utf-8')
        else:
            # åˆ›å»ºæ–°çš„æ¯æ—¥æ–‡ä»¶
            date_obj = date or datetime.now()
            existing = f"# {date_obj.strftime('%Y-%m-%d')} ä¼šè¯è®°å½•\n"

        # æŸ¥æ‰¾æˆ–åˆ›å»º section
        section_header = f"## {section}"
        if section_header in existing:
            # ä½¿ç”¨æ­£åˆ™åœ¨ section æœ«å°¾è¿½åŠ 
            import re
            # åŒ¹é… section header åˆ°ä¸‹ä¸€ä¸ª ## æˆ–æ–‡ä»¶æœ«å°¾
            pattern = f"({re.escape(section_header)}.*?)(\n## |$)"

            def replacer(match):
                section_content = match.group(1).rstrip()
                next_section = match.group(2)
                return f"{section_content}\n- {content}\n{next_section}"

            existing = re.sub(pattern, replacer, existing, count=1, flags=re.DOTALL)
        else:
            # åˆ›å»ºæ–° section
            existing = existing.rstrip() + f"\n\n{section_header}\n- {content}\n"

        filepath.write_text(existing, encoding='utf-8')
        logger.info(f"Appended to daily memory: {section}")

    def get_recent_daily(self, days: int = 2) -> str:
        """è·å–æœ€è¿‘å‡ å¤©çš„è®°å¿†ï¼ˆç”¨äºä¼šè¯å¼€å§‹æ—¶åŠ è½½ï¼‰"""
        memories = []
        today = datetime.now()

        for i in range(days):
            date = today - timedelta(days=i)
            content = self.load_daily(date)
            if content:
                memories.append(content)

        return "\n\n---\n\n".join(memories)

    def archive_old_daily(self, keep_days: int = 7):
        """å½’æ¡£æ—§çš„æ¯æ—¥è®°å¿†"""
        today = datetime.now()
        cutoff = today - timedelta(days=keep_days)

        archived_count = 0
        for filepath in self.daily_dir.glob("*.md"):
            if filepath.name == "archive":
                continue

            try:
                date_str = filepath.stem  # YYYY-MM-DD
                file_date = datetime.strptime(date_str, "%Y-%m-%d")

                if file_date < cutoff:
                    # ç§»åŠ¨åˆ°å½’æ¡£ç›®å½•
                    archive_path = self.archive_dir / filepath.name
                    filepath.rename(archive_path)
                    archived_count += 1
                    logger.info(f"Archived: {filepath.name}")
            except ValueError:
                continue

        if archived_count > 0:
            logger.info(f"Archived {archived_count} daily memory files")

        return archived_count

    # ============ Layer 2: ä¸­æœŸè®°å¿† (Learned) ============

    def save_learned(self, name: str, content: str, metadata: Optional[Dict] = None):
        """ä¿å­˜å­¦ä¹ åˆ°çš„æ¨¡å¼"""
        # è§„èŒƒåŒ–æ–‡ä»¶å
        safe_name = re.sub(r'[^\w\-]', '-', name.lower())
        filepath = self.learned_dir / f"{safe_name}.md"

        # æ„å»ºæ–‡ä»¶å†…å®¹
        header = f"# {name}\n\n"
        header += f"**æå–æ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d')}\n"

        if metadata:
            if "source" in metadata:
                header += f"**æ¥æº**: {metadata['source']}\n"
            if "tags" in metadata:
                header += f"**æ ‡ç­¾**: {', '.join(metadata['tags'])}\n"

        header += "\n"

        full_content = header + content
        filepath.write_text(full_content, encoding='utf-8')
        logger.info(f"Saved learned pattern: {filepath.name}")

        # æ›´æ–°ç´¢å¼•
        self._update_index()

        return filepath

    def load_learned(self, name: str) -> Optional[str]:
        """åŠ è½½æŒ‡å®šçš„å­¦ä¹ æ¨¡å¼"""
        safe_name = re.sub(r'[^\w\-]', '-', name.lower())
        filepath = self.learned_dir / f"{safe_name}.md"

        if filepath.exists():
            return filepath.read_text(encoding='utf-8')

        # å°è¯•æ¨¡ç³ŠåŒ¹é…
        for f in self.learned_dir.glob("*.md"):
            if safe_name in f.stem:
                return f.read_text(encoding='utf-8')

        return None

    def list_learned(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å­¦ä¹ çš„æ¨¡å¼"""
        patterns = []

        for filepath in self.learned_dir.glob("*.md"):
            stat = filepath.stat()

            # è¯»å–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
            content = filepath.read_text(encoding='utf-8')
            first_line = content.split('\n')[0] if content else ""
            title = first_line.lstrip('#').strip() if first_line.startswith('#') else filepath.stem

            patterns.append({
                "name": filepath.stem,
                "title": title,
                "path": str(filepath),
                "size": stat.st_size,
                "modified": datetime.fromtimestamp(stat.st_mtime)
            })

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        patterns.sort(key=lambda x: x["modified"], reverse=True)
        return patterns

    # ============ Layer 3: é•¿æœŸè®°å¿† (MEMORY.md) ============

    def load_memory(self) -> str:
        """åŠ è½½é•¿æœŸè®°å¿†"""
        if self.memory_file.exists():
            return self.memory_file.read_text(encoding='utf-8')
        return ""

    def save_memory(self, content: str):
        """ä¿å­˜é•¿æœŸè®°å¿†"""
        self.memory_file.write_text(content, encoding='utf-8')
        logger.info("Saved long-term memory")

    def _is_similar_content(self, existing_text: str, new_content: str, threshold: float = 0.6) -> bool:
        """æ£€æŸ¥æ–°å†…å®¹æ˜¯å¦ä¸å·²æœ‰å†…å®¹ç›¸ä¼¼ï¼ˆå­—ç¬¦çº§åˆ«çš„é‡å æ£€æµ‹ï¼‰"""
        import re

        def normalize(text):
            # å»é™¤æ ‡ç‚¹å’Œç©ºæ ¼ï¼Œåªä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
            return re.sub(r'[^\u4e00-\u9fff a-zA-Z0-9]', '', text.lower())

        def get_ngrams(text, n=2):
            """è·å– n-gram é›†åˆï¼ˆç”¨äºä¸­æ–‡çš„å­—ç¬¦çº§åˆ«åŒ¹é…ï¼‰"""
            text = normalize(text)
            if len(text) < n:
                return {text} if text else set()
            return set(text[i:i+n] for i in range(len(text) - n + 1))

        new_ngrams = get_ngrams(new_content)
        if not new_ngrams:
            return False

        # æ£€æŸ¥å·²æœ‰å†…å®¹çš„æ¯ä¸€è¡Œ
        for line in existing_text.split('\n'):
            if line.strip().startswith('- '):
                # æå–è®°å¿†æ¡ç›®å†…å®¹ï¼ˆå»é™¤æ—¶é—´æˆ³ï¼‰
                line_content = re.sub(r'\[\d{2}-\d{2}\]', '', line)
                line_content = re.sub(r'\[\d{4}-\d{2}-\d{2}.*?\]', '', line_content)
                line_ngrams = get_ngrams(line_content)
                if not line_ngrams:
                    continue
                # è®¡ç®— n-gram é‡å ç‡
                overlap = len(new_ngrams & line_ngrams) / len(new_ngrams)
                if overlap >= threshold:
                    return True
        return False

    def append_memory(self, section: str, content: str, check_duplicate: bool = True) -> bool:
        """è¿½åŠ åˆ°é•¿æœŸè®°å¿†çš„æŒ‡å®š section

        Args:
            section: åˆ†ç±»åç§°
            content: è®°å¿†å†…å®¹
            check_duplicate: æ˜¯å¦æ£€æŸ¥é‡å¤ï¼ˆé»˜è®¤ Trueï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸæ·»åŠ ï¼ˆå¦‚æœé‡å¤åˆ™è¿”å› Falseï¼‰
        """
        existing = self.load_memory()

        if not existing:
            existing = "# é•¿æœŸè®°å¿†\n\n"

        # P1 æ”¹è¿›: å»é‡æ£€æŸ¥
        if check_duplicate and self._is_similar_content(existing, content):
            logger.info(f"Skipped duplicate memory: {content[:50]}...")
            return False

        section_header = f"## {section}"
        timestamp = datetime.now().strftime("%m-%d")  # P2 æ”¹è¿›: ç®€åŒ–æ—¶é—´æˆ³
        entry = f"- [{timestamp}] {content}"

        if section_header in existing:
            # åœ¨ section æœ«å°¾è¿½åŠ 
            parts = existing.split(section_header)
            before = parts[0]
            after = parts[1]

            # æ‰¾åˆ°ä¸‹ä¸€ä¸ª ## æˆ–æ–‡ä»¶æœ«å°¾
            next_section = after.find('\n## ')
            if next_section == -1:
                # æ–‡ä»¶æœ«å°¾
                after = after.rstrip() + f"\n{entry}\n"
            else:
                # åœ¨ä¸‹ä¸€ä¸ª section å‰æ’å…¥
                after = after[:next_section].rstrip() + f"\n{entry}\n" + after[next_section:]

            existing = before + section_header + after
        else:
            # åˆ›å»ºæ–° section
            existing = existing.rstrip() + f"\n\n{section_header}\n{entry}\n"

        self.save_memory(existing)
        logger.info(f"Appended to memory section: {section}")
        return True

    # ============ ç´¢å¼•ç®¡ç† ============

    def _update_index(self):
        """æ›´æ–°è®°å¿†ç´¢å¼•æ–‡ä»¶"""
        content = "# è®°å¿†ç´¢å¼•\n\n"
        content += f"*è‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M')}*\n\n"

        # é•¿æœŸè®°å¿†æ‘˜è¦
        content += "## é•¿æœŸè®°å¿† (MEMORY.md)\n\n"
        if self.memory_file.exists():
            memory = self.load_memory()
            # æå– section æ ‡é¢˜
            sections = re.findall(r'^## (.+)$', memory, re.MULTILINE)
            for section in sections:
                content += f"- {section}\n"
        else:
            content += "*æš‚æ— é•¿æœŸè®°å¿†*\n"
        content += "\n"

        # å­¦ä¹ æ¨¡å¼åˆ—è¡¨
        content += "## å­¦ä¹ æ¨¡å¼ (learned/)\n\n"
        patterns = self.list_learned()
        if patterns:
            for p in patterns[:20]:  # æœ€å¤šæ˜¾ç¤º 20 ä¸ª
                content += f"- **{p['title']}** ({p['name']}.md) - {p['modified'].strftime('%Y-%m-%d')}\n"
        else:
            content += "*æš‚æ— å­¦ä¹ æ¨¡å¼*\n"
        content += "\n"

        # è¿‘æœŸæ¯æ—¥è®°å¿†
        content += "## è¿‘æœŸä¼šè¯ (daily/)\n\n"
        daily_files = sorted(self.daily_dir.glob("*.md"), reverse=True)[:7]
        if daily_files:
            for f in daily_files:
                if f.name != "archive":
                    content += f"- {f.stem}\n"
        else:
            content += "*æš‚æ— ä¼šè¯è®°å½•*\n"

        self.index_file.write_text(content, encoding='utf-8')
        logger.debug("Updated memory index")

    def load_index(self) -> str:
        """åŠ è½½è®°å¿†ç´¢å¼•"""
        if self.index_file.exists():
            return self.index_file.read_text(encoding='utf-8')
        self._update_index()
        return self.index_file.read_text(encoding='utf-8')

    # ============ æœç´¢åŠŸèƒ½ ============

    def search(self, keyword: str) -> List[Dict[str, Any]]:
        """æœç´¢æ‰€æœ‰è®°å¿†æ–‡ä»¶ï¼ŒæŒ‰åŒ¹é…æ•°é™åºæ’åˆ—"""
        results = []
        keyword_lower = keyword.lower()

        # æœç´¢æ‰€æœ‰ .md æ–‡ä»¶
        search_paths = [
            self.memory_file,
            *self.daily_dir.glob("*.md"),
            *self.learned_dir.glob("*.md")
        ]

        for filepath in search_paths:
            if not filepath.exists() or not filepath.is_file():
                continue

            try:
                content = filepath.read_text(encoding='utf-8')
                if keyword_lower in content.lower():
                    # æ‰¾åˆ°åŒ¹é…çš„è¡Œ
                    matches = []
                    for i, line in enumerate(content.split('\n'), 1):
                        if keyword_lower in line.lower():
                            matches.append({
                                "line": i,
                                "text": line.strip()[:100]  # æˆªæ–­
                            })

                    results.append({
                        "file": str(filepath.relative_to(self.memory_dir)),
                        "matches": matches[:5],  # æ¯ä¸ªæ–‡ä»¶æœ€å¤š 5 ä¸ªåŒ¹é…
                        "match_count": len(matches)  # æ€»åŒ¹é…æ•°ç”¨äºæ’åº
                    })
            except Exception as e:
                logger.error(f"Error searching {filepath}: {e}")

        # P1 æ”¹è¿›: æŒ‰åŒ¹é…æ•°é™åºæ’åˆ—
        results.sort(key=lambda x: x["match_count"], reverse=True)
        return results

    # ============ ä¼šè¯é›†æˆ ============

    def get_learned_summaries(self, max_count: int = 3) -> str:
        """è·å–æœ€è¿‘çš„ learned æ¨¡å¼æ‘˜è¦"""
        patterns = self.list_learned()[:max_count]
        if not patterns:
            return ""

        summaries = []
        for p in patterns:
            try:
                content = Path(p["path"]).read_text(encoding='utf-8')
                # æå–å‰ 200 å­—ç¬¦ä½œä¸ºæ‘˜è¦
                lines = content.split('\n')
                # è·³è¿‡æ ‡é¢˜å’Œå…ƒæ•°æ®ï¼Œå–å®é™…å†…å®¹
                summary_lines = []
                for line in lines:
                    if line.startswith('#') or line.startswith('**'):
                        continue
                    if line.strip():
                        summary_lines.append(line.strip())
                    if len('\n'.join(summary_lines)) > 150:
                        break
                summary = '\n'.join(summary_lines)[:150]
                summaries.append(f"**{p['title']}**: {summary}...")
            except Exception as e:
                logger.error(f"Error reading learned pattern {p['path']}: {e}")

        return '\n\n'.join(summaries)

    def get_session_context(self) -> str:
        """
        è·å–ä¼šè¯å¼€å§‹æ—¶éœ€è¦åŠ è½½çš„ä¸Šä¸‹æ–‡

        è¿”å›: MEMORY.md + ä»Šå¤©/æ˜¨å¤©çš„ daily + learned æ‘˜è¦ + index
        """
        context_parts = []

        # é•¿æœŸè®°å¿†
        memory = self.load_memory()
        if memory:
            context_parts.append("=== é•¿æœŸè®°å¿† ===\n" + memory)

        # è¿‘æœŸä¼šè¯
        recent = self.get_recent_daily(days=2)
        if recent:
            context_parts.append("=== è¿‘æœŸä¼šè¯ ===\n" + recent)

        # P2 æ”¹è¿›: åŠ è½½æœ€è¿‘çš„ learned æ¨¡å¼æ‘˜è¦
        learned_summaries = self.get_learned_summaries(max_count=3)
        if learned_summaries:
            context_parts.append("=== å¯ç”¨æŠ€æœ¯æ¨¡å¼ ===\n" + learned_summaries)

        # ç´¢å¼•æ‘˜è¦
        index = self.load_index()
        if index:
            context_parts.append("=== å¯ç”¨è®°å¿†ç´¢å¼• ===\n" + index)

        return "\n\n".join(context_parts)

    def save_session_summary(self, summary: str):
        """ä¿å­˜ä¼šè¯æ‘˜è¦åˆ°ä»Šå¤©çš„ daily æ–‡ä»¶"""
        today = datetime.now()
        filepath = self.get_daily_file(today)

        if filepath.exists():
            existing = filepath.read_text(encoding='utf-8')
            # è¿½åŠ åˆ°æœ«å°¾
            existing = existing.rstrip() + f"\n\n---\n\n{summary}\n"
        else:
            existing = f"# {today.strftime('%Y-%m-%d')} ä¼šè¯è®°å½•\n\n{summary}\n"

        filepath.write_text(existing, encoding='utf-8')
        logger.info("Saved session summary")

        # æ›´æ–°ç´¢å¼•
        self._update_index()

    # ============ çŠ¶æ€æŸ¥è¯¢ ============

    def get_status(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç³»ç»ŸçŠ¶æ€"""
        status = {
            "daily": {
                "count": len(list(self.daily_dir.glob("*.md"))),
                "today_exists": self.get_daily_file().exists(),
                "archive_count": len(list(self.archive_dir.glob("*.md")))
            },
            "learned": {
                "count": len(list(self.learned_dir.glob("*.md"))),
                "patterns": [p["title"] for p in self.list_learned()[:5]]
            },
            "memory": {
                "exists": self.memory_file.exists(),
                "size": self.memory_file.stat().st_size if self.memory_file.exists() else 0
            }
        }

        return status

    def format_status(self) -> str:
        """æ ¼å¼åŒ–çŠ¶æ€ä¸ºç”¨æˆ·å¯è¯»çš„æ–‡æœ¬"""
        status = self.get_status()

        text = "ğŸ“š **è®°å¿†ç³»ç»ŸçŠ¶æ€**\n\n"

        # çŸ­æœŸè®°å¿†
        text += "**çŸ­æœŸè®°å¿† (daily/)**\n"
        text += f"  - æ–‡ä»¶æ•°: {status['daily']['count']}\n"
        text += f"  - ä»Šå¤©: {'âœ… æœ‰è®°å½•' if status['daily']['today_exists'] else 'âŒ æš‚æ— '}\n"
        text += f"  - å½’æ¡£: {status['daily']['archive_count']} ä¸ªæ–‡ä»¶\n\n"

        # ä¸­æœŸè®°å¿†
        text += "**ä¸­æœŸè®°å¿† (learned/)**\n"
        text += f"  - æ¨¡å¼æ•°: {status['learned']['count']}\n"
        if status['learned']['patterns']:
            text += f"  - æœ€è¿‘: {', '.join(status['learned']['patterns'][:3])}\n"
        text += "\n"

        # é•¿æœŸè®°å¿†
        text += "**é•¿æœŸè®°å¿† (MEMORY.md)**\n"
        if status['memory']['exists']:
            text += f"  - å¤§å°: {status['memory']['size']} bytes\n"
        else:
            text += "  - âŒ å°šæœªåˆ›å»º\n"

        return text

    # ============ æŒç»­å­¦ä¹  - ä¼šè¯æ¨¡å¼æå– ============

    PATTERN_TYPES = {
        "error_resolution": "é”™è¯¯è§£å†³æ–¹æ¡ˆ",
        "user_correction": "ç”¨æˆ·çº æ­£",
        "workaround": "å˜é€šæ–¹æ¡ˆ",
        "debugging_technique": "è°ƒè¯•æŠ€å·§",
        "project_specific": "é¡¹ç›®ç‰¹å®šçŸ¥è¯†",
    }

    async def extract_patterns_from_session(
        self,
        session_transcript: str,
        session_name: str,
        min_length: int = 500
    ) -> List[Dict[str, Any]]:
        """
        ä»ä¼šè¯è®°å½•ä¸­æå–å¯å¤ç”¨çš„æ¨¡å¼

        Args:
            session_transcript: ä¼šè¯æ–‡æœ¬å†…å®¹
            session_name: ä¼šè¯åç§°
            min_length: æœ€å°æ–‡æœ¬é•¿åº¦ï¼ˆå¤ªçŸ­çš„ä¼šè¯ä¸åˆ†æï¼‰

        Returns:
            æå–çš„æ¨¡å¼åˆ—è¡¨
        """
        if len(session_transcript) < min_length:
            logger.info(f"ä¼šè¯å†…å®¹å¤ªçŸ­ ({len(session_transcript)}å­—)ï¼Œè·³è¿‡æ¨¡å¼æå–")
            return []

        # ä½¿ç”¨ Claude CLI åˆ†æä¼šè¯
        prompt = f"""åˆ†æä»¥ä¸‹ä¼šè¯è®°å½•ï¼Œæå–å¯å¤ç”¨çš„æŠ€æœ¯æ¨¡å¼ã€‚

ä¼šè¯åç§°: {session_name}

ä¼šè¯å†…å®¹:
{session_transcript[:8000]}  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡º token

è¯·è¯†åˆ«ä»¥ä¸‹ç±»å‹çš„æ¨¡å¼ï¼ˆåªè¾“å‡ºç¡®å®å­˜åœ¨çš„ï¼‰ï¼š
1. error_resolution - é”™è¯¯è§£å†³æ–¹æ¡ˆï¼šå¦‚ä½•è§£å†³äº†æŸä¸ªé”™è¯¯
2. user_correction - ç”¨æˆ·çº æ­£ï¼šç”¨æˆ·çº æ­£äº† AI çš„ä»€ä¹ˆç†è§£
3. workaround - å˜é€šæ–¹æ¡ˆï¼šç»•è¿‡æŸä¸ªé—®é¢˜çš„æŠ€å·§
4. debugging_technique - è°ƒè¯•æŠ€å·§ï¼šæœ‰æ•ˆçš„è°ƒè¯•æ–¹æ³•
5. project_specific - é¡¹ç›®ç‰¹å®šï¼šè¯¥é¡¹ç›®çš„ç‰¹æ®Šçº¦å®šæˆ–çŸ¥è¯†

è¾“å‡ºæ ¼å¼ï¼ˆJSON æ•°ç»„ï¼Œå¦‚æœæ²¡æœ‰å‘ç°ä»»ä½•æ¨¡å¼åˆ™è¾“å‡ºç©ºæ•°ç»„ []ï¼‰:
```json
[
  {{
    "type": "error_resolution",
    "title": "æ¨¡å¼æ ‡é¢˜ï¼ˆç®€çŸ­æè¿°ï¼‰",
    "context": "ä»€ä¹ˆæƒ…å†µä¸‹ä¼šé‡åˆ°è¿™ä¸ªé—®é¢˜",
    "solution": "è§£å†³æ–¹æ¡ˆçš„è¯¦ç»†æè¿°",
    "example": "ç›¸å…³ä»£ç ç¤ºä¾‹ï¼ˆå¦‚æœæœ‰ï¼‰"
  }}
]
```

é‡è¦ï¼šåªæå–çœŸæ­£æœ‰ä»·å€¼ã€å¯å¤ç”¨çš„æ¨¡å¼ã€‚æ™®é€šçš„å¯¹è¯ä¸éœ€è¦æå–ã€‚"""

        try:
            # è°ƒç”¨ Claude CLI
            result = subprocess.run(
                ['claude', '-p', prompt, '--model', 'haiku', '--output-format', 'json'],
                capture_output=True,
                text=True,
                timeout=60,
                cwd=str(self.data_dir.parent)
            )

            if result.returncode != 0:
                logger.error(f"Claude CLI æ‰§è¡Œå¤±è´¥: {result.stderr}")
                return []

            # è§£æè¾“å‡º
            output = result.stdout.strip()

            # å°è¯•ä»è¾“å‡ºä¸­æå– JSON
            patterns = self._parse_patterns_json(output)

            if patterns:
                logger.info(f"ä»ä¼šè¯ '{session_name}' æå–äº† {len(patterns)} ä¸ªæ¨¡å¼")

            return patterns

        except subprocess.TimeoutExpired:
            logger.warning("æ¨¡å¼æå–è¶…æ—¶")
            return []
        except Exception as e:
            logger.error(f"æ¨¡å¼æå–å¤±è´¥: {e}")
            return []

    def _parse_patterns_json(self, output: str) -> List[Dict[str, Any]]:
        """ä» Claude è¾“å‡ºä¸­è§£ææ¨¡å¼ JSON"""
        import json

        # å°è¯•ç›´æ¥è§£æ
        try:
            data = json.loads(output)
            if isinstance(data, dict) and "result" in data:
                # Claude CLI JSON æ ¼å¼
                result_text = data.get("result", "")
                return self._extract_json_from_text(result_text)
            elif isinstance(data, list):
                return data
        except json.JSONDecodeError:
            pass

        # ä»æ–‡æœ¬ä¸­æå– JSON
        return self._extract_json_from_text(output)

    def _extract_json_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå– JSON æ•°ç»„"""
        import json

        # æŸ¥æ‰¾ JSON æ•°ç»„
        match = re.search(r'\[\s*\{.*?\}\s*\]', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group())
            except json.JSONDecodeError:
                pass

        # æŸ¥æ‰¾ç©ºæ•°ç»„
        if '[]' in text:
            return []

        return []

    def save_extracted_patterns(self, patterns: List[Dict[str, Any]], source: str):
        """ä¿å­˜æå–çš„æ¨¡å¼åˆ° learned/ ç›®å½•"""
        saved_count = 0

        for pattern in patterns:
            pattern_type = pattern.get("type", "unknown")
            title = pattern.get("title", "æœªå‘½åæ¨¡å¼")

            # æ„å»ºå†…å®¹
            content = f"## åœºæ™¯\n{pattern.get('context', 'æ— ')}\n\n"
            content += f"## è§£å†³æ–¹æ¡ˆ\n{pattern.get('solution', 'æ— ')}\n\n"

            if pattern.get("example"):
                content += f"## ç¤ºä¾‹\n```\n{pattern['example']}\n```\n"

            # å…ƒæ•°æ®
            metadata = {
                "source": source,
                "tags": [pattern_type, self.PATTERN_TYPES.get(pattern_type, pattern_type)]
            }

            # ä¿å­˜
            self.save_learned(title, content, metadata)
            saved_count += 1

        logger.info(f"ä¿å­˜äº† {saved_count} ä¸ªæå–çš„æ¨¡å¼")
        return saved_count

    async def learn_from_session(self, session_transcript: str, session_name: str) -> int:
        """
        ä»ä¼šè¯ä¸­å­¦ä¹ ï¼ˆå®Œæ•´æµç¨‹ï¼‰

        Args:
            session_transcript: ä¼šè¯å†…å®¹
            session_name: ä¼šè¯åç§°

        Returns:
            æå–å¹¶ä¿å­˜çš„æ¨¡å¼æ•°é‡
        """
        patterns = await self.extract_patterns_from_session(session_transcript, session_name)

        if patterns:
            return self.save_extracted_patterns(patterns, f"session:{session_name}")

        return 0

    # ============ è®°å¿†é—å¿˜æœºåˆ¶ ============

    def calculate_memory_score(self, filepath: Path) -> Tuple[float, Dict[str, Any]]:
        """
        è®¡ç®—è®°å¿†æ–‡ä»¶çš„ä»·å€¼åˆ†æ•°

        è¯„åˆ†å› ç´ :
        - æ—¶é—´è¡°å‡: è¶Šæ—§åˆ†æ•°è¶Šä½
        - æ–‡ä»¶å¤§å°: å¤ªå°å¯èƒ½ä»·å€¼ä½
        - è®¿é—®é¢‘ç‡: ä» git log æˆ– atime åˆ¤æ–­ï¼ˆç®€åŒ–ç‰ˆç”¨ mtimeï¼‰

        Returns:
            (score, details)
        """
        if not filepath.exists():
            return 0.0, {"reason": "æ–‡ä»¶ä¸å­˜åœ¨"}

        stat = filepath.stat()
        now = datetime.now()

        # 1. æ—¶é—´å› ç´  (0-40åˆ†)
        modified = datetime.fromtimestamp(stat.st_mtime)
        days_old = (now - modified).days

        if days_old <= 7:
            time_score = 40
        elif days_old <= 30:
            time_score = 30
        elif days_old <= 90:
            time_score = 20
        elif days_old <= 180:
            time_score = 10
        else:
            time_score = 5

        # 2. å¤§å°å› ç´  (0-30åˆ†)
        size = stat.st_size
        if size < 100:
            size_score = 5  # å¤ªå°ï¼Œå¯èƒ½æ²¡ä»·å€¼
        elif size < 500:
            size_score = 15
        elif size < 2000:
            size_score = 30
        else:
            size_score = 25  # å¤ªå¤§å¯èƒ½æ˜¯æ‚ä¹±çš„

        # 3. å†…å®¹è´¨é‡å› ç´  (0-30åˆ†)
        try:
            content = filepath.read_text(encoding='utf-8')
            quality_score = self._evaluate_content_quality(content)
        except Exception:
            quality_score = 10

        total_score = time_score + size_score + quality_score

        details = {
            "days_old": days_old,
            "size": size,
            "time_score": time_score,
            "size_score": size_score,
            "quality_score": quality_score,
            "total": total_score
        }

        return total_score, details

    def _evaluate_content_quality(self, content: str) -> int:
        """è¯„ä¼°å†…å®¹è´¨é‡ (0-30åˆ†)"""
        score = 10  # åŸºç¡€åˆ†

        # æœ‰ç»“æ„åŒ–æ ‡é¢˜
        if re.search(r'^##?\s', content, re.MULTILINE):
            score += 5

        # æœ‰ä»£ç å—
        if '```' in content:
            score += 5

        # æœ‰å®é™…å†…å®¹ï¼ˆä¸åªæ˜¯æ ‡é¢˜å’Œå…ƒæ•°æ®ï¼‰
        lines = [l for l in content.split('\n') if l.strip() and not l.startswith('#') and not l.startswith('**')]
        if len(lines) > 5:
            score += 5

        # æœ‰å…³é”®è¯è¡¨æ˜æ˜¯æœ‰ä»·å€¼çš„æŠ€æœ¯å†…å®¹
        valuable_keywords = ['è§£å†³', 'ä¿®å¤', 'fix', 'error', 'é…ç½®', 'å‘½ä»¤', 'API', 'åŸå› ']
        if any(kw in content.lower() for kw in valuable_keywords):
            score += 5

        return min(score, 30)

    def get_forgettable_memories(
        self,
        threshold: float = 30.0,
        max_count: int = 10
    ) -> List[Dict[str, Any]]:
        """
        è·å–å¯ä»¥é—å¿˜çš„è®°å¿†æ–‡ä»¶

        Args:
            threshold: åˆ†æ•°ä½äºæ­¤é˜ˆå€¼çš„è¢«è®¤ä¸ºå¯é—å¿˜
            max_count: æœ€å¤šè¿”å›å¤šå°‘ä¸ª

        Returns:
            å¯é—å¿˜çš„è®°å¿†åˆ—è¡¨
        """
        candidates = []

        # æ£€æŸ¥ learned/ ç›®å½•
        for filepath in self.learned_dir.glob("*.md"):
            score, details = self.calculate_memory_score(filepath)

            if score < threshold:
                candidates.append({
                    "path": str(filepath),
                    "name": filepath.stem,
                    "score": score,
                    "details": details
                })

        # æ£€æŸ¥ daily/archive/ ç›®å½•ï¼ˆæ—§çš„æ¯æ—¥è®°å¿†ï¼‰
        for filepath in self.archive_dir.glob("*.md"):
            score, details = self.calculate_memory_score(filepath)

            # å½’æ¡£çš„æ—¥å¿—ç”¨æ›´ä½çš„é˜ˆå€¼
            if score < threshold * 0.7:
                candidates.append({
                    "path": str(filepath),
                    "name": f"archive/{filepath.stem}",
                    "score": score,
                    "details": details
                })

        # æŒ‰åˆ†æ•°å‡åºæ’åˆ—ï¼ˆæœ€ä½åˆ†çš„æœ€å®¹æ˜“è¢«é—å¿˜ï¼‰
        candidates.sort(key=lambda x: x["score"])

        return candidates[:max_count]

    def forget(
        self,
        memory_path: Optional[str] = None,
        auto: bool = False,
        threshold: float = 25.0,
        dry_run: bool = True
    ) -> List[str]:
        """
        é—å¿˜ï¼ˆåˆ é™¤ï¼‰ä½ä»·å€¼è®°å¿†

        Args:
            memory_path: æŒ‡å®šè¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„
            auto: è‡ªåŠ¨æ¨¡å¼ï¼Œåˆ é™¤æ‰€æœ‰ä½äºé˜ˆå€¼çš„è®°å¿†
            threshold: è‡ªåŠ¨æ¨¡å¼çš„é˜ˆå€¼
            dry_run: åªé¢„è§ˆä¸å®é™…åˆ é™¤

        Returns:
            è¢«åˆ é™¤ï¼ˆæˆ–å°†è¢«åˆ é™¤ï¼‰çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        deleted = []

        if memory_path:
            # åˆ é™¤æŒ‡å®šæ–‡ä»¶
            path = Path(memory_path)
            if path.exists():
                if not dry_run:
                    path.unlink()
                    logger.info(f"å·²åˆ é™¤è®°å¿†: {path}")
                deleted.append(str(path))

        elif auto:
            # è‡ªåŠ¨åˆ é™¤ä½åˆ†è®°å¿†
            candidates = self.get_forgettable_memories(threshold=threshold, max_count=20)

            for item in candidates:
                path = Path(item["path"])
                if path.exists():
                    if not dry_run:
                        path.unlink()
                        logger.info(f"è‡ªåŠ¨é—å¿˜: {path} (åˆ†æ•°: {item['score']:.1f})")
                    deleted.append(item["path"])

        if deleted and not dry_run:
            # æ›´æ–°ç´¢å¼•
            self._update_index()

        return deleted

    def get_forget_preview(self, threshold: float = 25.0) -> str:
        """
        è·å–é—å¿˜é¢„è§ˆï¼ˆç”¨æˆ·å‹å¥½çš„æ ¼å¼ï¼‰
        """
        candidates = self.get_forgettable_memories(threshold=threshold)

        if not candidates:
            return "æ²¡æœ‰å‘ç°å¯ä»¥é—å¿˜çš„ä½ä»·å€¼è®°å¿†ã€‚"

        text = f"å‘ç° {len(candidates)} ä¸ªä½ä»·å€¼è®°å¿†ï¼ˆåˆ†æ•° < {threshold}ï¼‰:\n\n"

        for item in candidates:
            text += f"â€¢ **{item['name']}** (åˆ†æ•°: {item['score']:.0f})\n"
            text += f"  - å¤©æ•°: {item['details']['days_old']}å¤©\n"
            text += f"  - å¤§å°: {item['details']['size']}å­—èŠ‚\n"

        text += "\nä½¿ç”¨ `/memory forget --confirm` æ‰§è¡Œé—å¿˜ã€‚"

        return text


# å…¨å±€å®ä¾‹
memory_manager: Optional[MemoryManager] = None


def init_memory(data_dir: str):
    """åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨"""
    global memory_manager
    memory_manager = MemoryManager(data_dir)
    logger.info(f"Memory manager initialized: {data_dir}/memory")
    return memory_manager


def get_memory_manager() -> Optional[MemoryManager]:
    """è·å–è®°å¿†ç®¡ç†å™¨å®ä¾‹"""
    return memory_manager
